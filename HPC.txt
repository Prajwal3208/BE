Here is the **detailed theory** with **all important points** for each implementation: CUDA Vector Addition, BFS/DFS using OpenMP, and Bubble/Merge Sort using OpenMP.

---

## ðŸ”¹ 1. **Vector Addition using CUDA**

**Definition**:
Vector addition computes element-wise sum of two vectors:
`C[i] = A[i] + B[i]` for all `i`.

**Important Points**:

* **Parallelization**: Each GPU thread computes one element independently â†’ data parallelism.
* **Memory Management**:

  * `cudaMalloc()` to allocate device memory.
  * `cudaMemcpy()` to transfer data from host to device and back.
  * `cudaFree()` to free device memory.
* **Kernel Launch**: `vectorAdd<<<blocks, threads>>>(...)`

  * `blocks` and `threads` define how many threads execute concurrently.
* **Advantages**:

  * Massively parallel.
  * Faster execution for large vectors compared to CPU.
* **Applications**:

  * Graphics, signal processing, physics simulations.

---

## ðŸ”¹ 2. **Breadth-First Search (BFS) using OpenMP**

**Definition**:
BFS explores the graph level-by-level using a queue.

**Important Points**:

* **Data Structures**: Queue, visited array, adjacency list.
* **Parallelism Strategy**:

  * Process all neighbors of nodes in current level in parallel using `#pragma omp parallel for`.
* **Race Conditions**: Need to synchronize access to the queue and visited array.
* **Barriers**: Used to wait for all threads to finish before moving to the next level.
* **Advantages**:

  * Efficient for wide graphs.
  * Better parallel performance than DFS.
* **Applications**:

  * Shortest path in unweighted graphs, web crawling, AI search.

---

## ðŸ”¹ 3. **Depth-First Search (DFS) using OpenMP**

**Definition**:
DFS explores graph by going deep along each branch before backtracking.

**Important Points**:

* **Recursive Approach**: Can be parallelized using OpenMP tasks.
* **OpenMP Usage**:

  * `#pragma omp parallel` to begin parallel region.
  * `#pragma omp single` to allow only one thread to begin.
  * `#pragma omp task` to spawn new thread for each child node.
* **Synchronization**:

  * `#pragma omp taskwait` to wait for all spawned tasks.
* **Challenges**:

  * Stack-based nature makes parallelism complex.
* **Applications**:

  * Pathfinding, solving puzzles, cycle detection.

---

## ðŸ”¹ 4. **Bubble Sort using OpenMP**

**Definition**:
A simple sorting method that repeatedly swaps adjacent elements if out of order.

**Important Points**:

* **Time Complexity**: O(nÂ²), not efficient for large arrays.
* **Parallel Strategy**:

  * Use **odd-even transposition**:

    * Odd phase: compare (1,2), (3,4), ...
    * Even phase: compare (0,1), (2,3), ...
  * Parallelize using `#pragma omp parallel for`.
* **Data Dependency**:

  * No overlapping index pairs during a phase, enabling safe parallelism.
* **Applications**:

  * Educational use, small datasets.

---

## ðŸ”¹ 5. **Merge Sort using OpenMP**

**Definition**:
Divide array into halves, recursively sort them, and merge the results.

**Important Points**:

* **Time Complexity**: O(n log n) â€“ efficient.
* **Divide-and-Conquer**:

  * Breaks the problem into independent subproblems.
* **OpenMP Tasks**:

  * `#pragma omp parallel`
  * `#pragma omp single`
  * `#pragma omp task` to sort each half in parallel.
* **Merging**: Done sequentially or with further optimization.
* **Scalability**: Good for large datasets due to task-based parallelism.
* **Applications**:

  * General-purpose sorting, external sorting for large files.

---

Would you like these in a **PDF/DOCX report format with a table of contents** and diagrams for each algorithm?

Great! Here's how you can **confidently answer oral viva-style questions** about **parallel computing**, **threading**, and **complexities** in CUDA and OpenMP programs:

---

## âœ… **1. How do you know your program is parallel?**

**Answer**:

* If the program uses **multiple threads or tasks** that execute **independent operations simultaneously**, it's parallel.
* In **OpenMP**, I use directives like `#pragma omp parallel`, `#pragma omp for`, and `#pragma omp task`.
* In **CUDA**, I use a **kernel function** with many threads (`<<<blocks, threads>>>`) where each thread performs one part of the work.
* **Example**: In CUDA vector addition, each thread adds one element of the vectors â†’ 1000 threads work at once.

---

## âœ… **2. What is parallel computing?**

**Answer**:

* Parallel computing means breaking a task into smaller parts and executing them **at the same time** using **multiple processors or threads**.
* It speeds up processing and is used in large computations, graphics, simulations, etc.

---

## âœ… **3. What is threading?**

**Answer**:

* Threading is the process of creating **multiple flows of execution** (called threads) within a program.
* Each thread can run a portion of code independently.
* In OpenMP, threads are managed by the compiler with `#pragma` directives.
* In CUDA, each thread on GPU runs part of the kernel function.

---

## âœ… **4. How do you implement parallelism in OpenMP?**

**Answer**:

* I include the OpenMP header: `#include <omp.h>`
* I use **pragmas** like:

  * `#pragma omp parallel` â†’ to create a team of threads.
  * `#pragma omp for` â†’ to divide loop iterations among threads.
  * `#pragma omp task` â†’ to run functions or recursion in parallel.
* I control threads with `omp_get_num_threads()` and `omp_get_thread_num()`.

---

## âœ… **5. How do you implement parallelism in CUDA?**

**Answer**:

* I define a **kernel function** using `__global__`.
* Launch with: `kernel<<<numBlocks, numThreads>>>(args);`
* Each thread uses:

  ```cpp
  int i = threadIdx.x + blockDim.x * blockIdx.x;
  ```

  to compute its own index and operate independently.
* Allocate memory with `cudaMalloc()` and copy data with `cudaMemcpy()`.

---

## âœ… **6. What are the complexities of the algorithms?**

| Algorithm       | Sequential Time Complexity | Parallel Time Complexity | Notes                              |
| --------------- | -------------------------- | ------------------------ | ---------------------------------- |
| **Vector Add**  | O(n)                       | O(n/p)                   | p = number of threads              |
| **BFS**         | O(V + E)                   | Reduced (level-parallel) | Parallel by level traversal        |
| **DFS**         | O(V + E)                   | Reduced (task-based)     | Less scalable than BFS in OpenMP   |
| **Bubble Sort** | O(nÂ²)                      | O(nÂ²/p)                  | Uses odd-even transposition method |
| **Merge Sort**  | O(n log n)                 | O((n log n)/p)           | Recursive parts are parallelized   |

---

## âœ… **7. How is memory managed in CUDA?**

**Answer**:

* Use `cudaMalloc()` to allocate memory on GPU.
* Use `cudaMemcpy()` to transfer data from CPU to GPU and back.
* Use `cudaFree()` to release memory.
* Memory types: Global memory, shared memory (fast, thread block-level), and registers.

---

## âœ… **8. What are barriers or synchronization?**

**Answer**:

* Used to ensure all threads complete a section before moving on.
* In **OpenMP**: `#pragma omp barrier`.
* In **CUDA**: `__syncthreads()` (used within a block).
* Prevents race conditions and ensures correctness.

---

## âœ… **9. What are race conditions?**

**Answer**:

* When two or more threads **access the same data at the same time**, and at least one thread **writes**, the output can be wrong.
* Prevented using synchronization (mutex, atomic, critical sections).

---

## âœ… **10. When is parallelism beneficial?**

**Answer**:

* When:

  * Tasks are **independent**.
  * There's a **large amount of data** (like big arrays).
  * Tasks can be **divided** with **minimal communication**.

---

Would you like a **PDF cheat-sheet** with all these answers for last-minute viva revision?

