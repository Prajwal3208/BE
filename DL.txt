housing
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score
import matplotlib.pyplot as plt
import seaborn as sns
 

data = pd.read_csv('Boston.csv')
 

# Assume the target column is named 'MEDV'
X = data.drop("medv", axis=1)
Y = data["medv"]
 

data.head()
 

data.shape
 

data.describe()
 

 

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
 

X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=42)
 

model = Sequential()
model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))
model.add(Dense(64, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(1))
 

model.compile(loss='mse', optimizer='adam', metrics=['mae'])
 

history = model.fit(X_train, Y_train, epochs=100, batch_size=1, verbose=1, validation_data=(X_test, Y_test))
 

mse = model.evaluate(X_test, Y_test)
print("Mean Squared Error:", mse)
 

y_pred = model.predict(X_test)
print(y_pred[:5])
 

plt.scatter(Y_test, y_pred, alpha=0.6)
plt.plot([Y_test.min(), Y_test.max()], [Y_test.min(), Y_test.max()], 'r--', lw=2)
plt.title('Predicted vs Actual Prices')
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.xlim([0, 60])
plt.ylim([0, 60])
plt.grid()
plt.show()


*****
Here's a short and simple explanation of your code:

---

### ðŸ  **Boston Housing Price Prediction Using Neural Network**

1. **Imports Libraries**: You import required tools for data handling (Pandas, NumPy), plotting (Matplotlib, Seaborn), machine learning (Scikit-learn), and deep learning (TensorFlow/Keras).

2. **Load Data**: You load the Boston housing dataset from `'Boston.csv'`.

3. **Split Features & Target**:

   * `X` = all features (input variables).
   * `Y` = target column (`medv`, which is the house price).

4. **Scale Features**: Normalize the feature values using `StandardScaler` to help the model train better.

5. **Split Data**: 80% for training and 20% for testing.

6. **Build Neural Network**:

   * 3 hidden layers with 128, 64, and 32 neurons using ReLU activation.
   * 1 output layer for predicting price.

7. **Compile Model**: Using Mean Squared Error (MSE) loss and Adam optimizer.

8. **Train Model**: You train for 100 epochs using the training data.

9. **Evaluate Model**: Calculates MSE and MAE (mean absolute error) on test data.

10. **Make Predictions**: Predicts prices on test data.

11. **Plot Results**: Shows a scatter plot comparing predicted vs actual house prices.

---




------------------------------------------------------------------------------------------------------------------------
imdb_dataset
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
import matplotlib.pyplot as plt
 

vocab_size = 10000
maxlen = 200
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)
 

x_train = pad_sequences(x_train, maxlen=maxlen)
x_test = pad_sequences(x_test, maxlen=maxlen)
 

model = keras.Sequential([
    layers.Embedding(input_dim=vocab_size, output_dim=32, input_length=maxlen),
    layers.GlobalAveragePooling1D(),
    layers.Dense(64, activation='relu'),
    layers.Dense(1, activation='sigmoid')
])
 

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
 

history = model.fit(x_train, y_train, epochs=10, batch_size=512, validation_split=0.2, verbose=1)
 

loss, accuracy = model.evaluate(x_test, y_test, verbose=1)
print(f"Test Accuracy: {accuracy:.4f}")
 

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
 

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
 

y_pred_probs = model.predict(x_test[:10])
y_pred_classes = (y_pred_probs > 0.5).astype("int32")
 

for i in range(10):
    print(f"Review {i+1} - Predicted: {'Positive' if y_pred_classes[i][0] == 1 else 'Negative'}, Actual: {'Positive' if y_test[i] == 1 else 'Negative'}")
 
***** Here's a short and simple explanation of your IMDb sentiment analysis code:

---

### ðŸŽ¬ **IMDb Movie Review Sentiment Classifier**

1. **Imports Libraries**: You load TensorFlow and tools for building neural networks and plotting.

2. **Load Dataset**:
   You load 50,000 preprocessed movie reviews:

   * `x_train`, `y_train`: training data
   * `x_test`, `y_test`: test data
   * Each review is encoded as integers (words replaced with numbers).

3. **Pad Sequences**:
   All reviews are made the same length (`maxlen = 200`) using `pad_sequences()`.

4. **Build Model**:

   * **Embedding Layer**: Turns word indices into vectors (like word meaning).
   * **GlobalAveragePooling1D**: Averages word vectors into one vector.
   * **Dense Layers**: Fully connected layers; the last layer outputs 0â€“1 (negative to positive sentiment).

5. **Compile Model**:
   Uses:

   * `adam` optimizer
   * `binary_crossentropy` loss (good for yes/no classification)
   * Tracks `accuracy`.

6. **Train Model**:
   Trains for 10 epochs using 80% of training data (20% for validation).

7. **Evaluate Model**:
   Prints test accuracy.

8. **Plot Accuracy & Loss**:
   Shows how accuracy and loss change over epochs.

9. **Make Predictions**:

   * Predicts first 10 reviews.
   * Prints whether each is predicted as *Positive* or *Negative* and compares to actual.

---

Let me know if you want a simpler version or explanation of specific layers.

-------------------------------------------------------------------------------------------------------------------------
fashion
 

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import numpy as np
 

fashion_mnist = keras.datasets.fashion_mnist
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()
 

x_train = x_train / 255.0
x_test = x_test / 255.0
 

class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
 

model = keras.Sequential([
    layers.Reshape((28, 28, 1), input_shape=(28, 28)),
    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D(2, 2),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D(2, 2),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(10, activation='softmax')
])
 

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
 

history = model.fit(x_train, y_train, epochs=10, validation_split=0.2)
 

test_loss, test_acc = model.evaluate(x_test, y_test)
print(f"Test accuracy: {test_acc:.4f}")
 

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
 

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
 

predictions = model.predict(x_test[:5])
 

for i in range(5):
    plt.figure(figsize=(2, 2))
    plt.imshow(x_test[i], cmap='gray')
    plt.title(f"Pred: {class_names[np.argmax(predictions[i])]}\nActual: {class_names[y_test[i]]}", fontsize=8)
    plt.axis('off')
    plt.tight_layout()
    plt.show()
 ***
Here's a short and simple explanation of your Fashion MNIST CNN model code:

---

### ðŸ‘• **Fashion Item Classifier (Image Recognition)**

1. **Load Data**:
   Gets fashion images (28Ã—28 grayscale) from the Fashion MNIST dataset.
   Each image belongs to one of 10 categories like T-shirt, Shoe, Bag, etc.

2. **Normalize Images**:
   Pixel values are scaled from 0â€“255 to 0â€“1 for better training.

3. **Class Labels**:
   A list that maps numbers (0â€“9) to clothing item names.

4. **Build CNN Model**:

   * **Reshape**: Adds a channel dimension (28Ã—28Ã—1).
   * **Conv2D + MaxPooling**: Extracts image features and reduces size.
   * **Flatten**: Converts 2D to 1D.
   * **Dense Layers**: Fully connected layers for classification.
   * **Softmax**: Outputs probabilities for each class (10 total).

5. **Compile Model**:
   Uses Adam optimizer and sparse categorical loss (for integer labels).

6. **Train Model**:
   Trains for 10 epochs with 80% training data and 20% for validation.

7. **Evaluate Model**:
   Tests accuracy on unseen test images and prints it.

8. **Plot Accuracy & Loss**:
   Shows how training and validation accuracy/loss change over time.

9. **Predict and Display Images**:
   Shows 5 test images with predicted and actual clothing names.

---

Want help turning this into a project report or explanation slide?


--------------------------------------------------------------------------------------------------------------------------
stock_price
 

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from sklearn.preprocessing import MinMaxScaler
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
 

data = pd.read_csv('GOOGL.csv')
 

data = data[["Close"]]
data.dropna(inplace=True)
 

scaler = MinMaxScaler(feature_range=(0, 1))
data_scaled = scaler.fit_transform(data)
 

def create_sequences(data, sequence_length):
    X, Y = [], []
    for i in range(sequence_length, len(data)):
        X.append(data[i-sequence_length:i, 0])
        Y.append(data[i, 0])
    return np.array(X), np.array(Y)
 

sequence_length = 60
X, Y = create_sequences(data_scaled, sequence_length)
 

X = np.reshape(X, (X.shape[0], X.shape[1], 1))
 

split = int(0.8 * len(X))
X_train, X_test = X[:split], X[split:]
Y_train, Y_test = Y[:split], Y[split:]
 

model = keras.Sequential([
    layers.SimpleRNN(50, return_sequences=True, input_shape=(X_train.shape[1], 1)),
    layers.SimpleRNN(50),
    layers.Dense(1)
])
 

model.compile(optimizer='adam', loss='mean_squared_error')
 

history = model.fit(X_train, Y_train, epochs=50, batch_size=32, validation_split=0.1, verbose=1)
 

loss = model.evaluate(X_test, Y_test)
print(f"Test Loss: {loss:.4f}")
 

predicted_stock_price = model.predict(X_test)
 

predicted_stock_price = scaler.inverse_transform(predicted_stock_price.reshape(-1, 1))
y_test_scaled = scaler.inverse_transform(Y_test.reshape(-1, 1))
 

plt.figure(figsize=(10, 6))
plt.plot(y_test_scaled, color='blue', label='Actual Google Stock Price')
plt.plot(predicted_stock_price, color='red', label='Predicted Google Stock Price')
plt.title('Google Stock Price Prediction')
plt.xlabel('Time')
plt.ylabel('Stock Price')
plt.legend()
plt.show()

Hereâ€™s a short and simple explanation of your **stock price prediction** code:

---

### ðŸ“ˆ **Google Stock Price Prediction Using RNN**

1. **Load Data**:
   You load only the "Close" prices from `GOOGL.csv`.

2. **Scale Data**:
   Use `MinMaxScaler` to scale prices between 0 and 1 (helps the model train faster).

3. **Create Sequences**:
   For each point, you take the last 60 days of prices as input (`X`) and the next day's price as output (`Y`).

4. **Reshape Data**:
   Format input data to fit RNN: shape = `(samples, 60, 1)`.

5. **Split Data**:
   80% is used for training, 20% for testing.

6. **Build RNN Model**:

   * Uses **2 SimpleRNN layers**.
   * Ends with a **Dense** layer to predict the price.
   * `return_sequences=True` allows stacking RNN layers.

7. **Train Model**:
   Model trains for 50 epochs with a batch size of 32.

8. **Evaluate & Predict**:
   Model evaluates test loss and predicts future prices.

9. **Inverse Transform**:
   Converts scaled predictions back to actual prices.

10. **Plot Results**:
    Shows a graph comparing predicted vs. actual Google stock prices.

---

Let me know if you want this rewritten for a project report or poster!

 
